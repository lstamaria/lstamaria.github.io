# PyCaret

Automated machine learning is on the rise with the likes of AutoSklearn, AutoKeras, and H2O. Adding to this list of Python packages is PyCaret. Although it misses on exposing the data output of model evaluation, PyCaret is promising because it provides ease of coding and end-to-end view of data science work.

But first, let's check the Pycaret version used in this article.


```python
!pip3 list | grep pycaret
```

    pycaret                   2.1.2
    pycaret-nightly           2.2.dev1604190367


Although the latest PyCaret version is 2.2.3, we use an older version in order to satisfy the development environment that we currently have. You might need to experiment a little on other environments if you wish to test the latest version.

## One primary advantage of Pycaret is the ease of coding. 
Pycaret code is writable and readable. Writable means that the code is relatively short. For example, to read sample data from PyCaret:


```python
from pycaret.datasets import get_data
data = get_data(dataset="juice")
```


![png](/assets/images/20210105/line66.png)



```python
data.shape
```




    (1070, 19)




```python
data["Purchase"].unique()
```




    array(['CH', 'MM'], dtype=object)



In this example, we use a sample data named `juice` from PyCaret with 1070 rows and 19 columns. The target column is `Purchase` that appears as binary in notation: `CH` and `MM`. From this, we can assume that our domain problem involves classification. The line below calls all the necessary tools needed to perform classification modeling. Such is the power of using PyCaret.


```python
from pycaret.classification import * 
```

From here, we can proceed right away to modeling. (Note: It is important to perform data understanding first before proceeding to modeling. Data understanding gives us an deeper exploration of the problem that we are trying to solve. For now, we will skip that part.)

Pycaret code is also readable aside from being writable. It uses simple English words to convey a function's purpose. Using the sample data, we prepare our data ready for modeling by calling `setup`. We will do this by assigning this step as `init` or any contextual name. As easy as that.

The `setup` also includes built-in preprocessing. This saves a lot of time in exploring multiple techniques such as imputation of missing values and one-hot encoding of categorical features.


```python
init = setup(data=data, target="Purchase")
```


![png](/assets/images/20210105/line70.png)


PyCaret applies abstraction which means that it hides unncessary code from the user. This helps especially beginners to speed up their understanding of modeling steps. In the succeeding line, we call `compare_models` to create multiple models, sort them based on recall metric, and select top 2 models only. We assign the result to `best`.


```python
top_model = compare_models(sort="recall", n_select=2)
```


![png](/assets/images/20210105/line71.png)


Using `compare_models` allows us to do multiple tasks right away. We call our champion model as `top_model[0]` and challenger model as `top_model[1]`.


```python
top_model[0]
```




    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                       intercept_scaling=1, l1_ratio=None, max_iter=1000,
                       multi_class='auto', n_jobs=None, penalty='l2',
                       random_state=3800, solver='lbfgs', tol=0.0001, verbose=0,
                       warm_start=False)




```python
top_model[1]
```




    RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                    max_iter=None, normalize=False, random_state=3800,
                    solver='auto', tol=0.001)



## Pycaret gives an end-to-end view of data science work. 
Its documentation supports the necessary stages of CRISP-DM framework. The Cross-Industry Process for Data Mining (CRISP-DM) is a popular framework used in data science. Pycaret involves data science work such as modeling, evaluation, and deployment. 

The previous section shows how Pycaret supports modeling. As a continuation, we demonstrate how Pycaret provides evaluation and deployment in the succeeding section.

We use `predict_model` to predict new data after building a model. The data used is the test set which came from `setup` after a default 70-30 split.


```python
champion_holdout = predict_model(estimator=top_model[0], probability_threshold=0.5)
```


![png](/assets/images/20210105/line75.png)



```python
challenger_holdout = predict_model(estimator=top_model[1], probability_threshold=0.5)
```


![png](/assets/images/20210105/line76.png)


After a quick assessment of our models, we would like to fit the champion model to the complete dataset. We can do this using `finalize_model` as the final step of experimentation.


```python
finalize_model(estimator=top_model[0])
```




    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                       intercept_scaling=1, l1_ratio=None, max_iter=1000,
                       multi_class='auto', n_jobs=None, penalty='l2',
                       random_state=3800, solver='lbfgs', tol=0.0001, verbose=0,
                       warm_start=False)



Finally, we deploy the champion model. To do this, we call `save_model` and `load_model` to save and load the champion model, respectively.


```python
save_model(model=top_model[0], model_name="champion_model_20201231")
```

    Transformation Pipeline and Model Succesfully Saved





    (Pipeline(memory=None,
              steps=[('dtypes',
                      DataTypes_Auto_infer(categorical_features=[],
                                           display_types=True, features_todrop=[],
                                           id_columns=['Id'],
                                           ml_usecase='classification',
                                           numerical_features=[], target='Purchase',
                                           time_features=[])),
                     ('imputer',
                      Simple_Imputer(categorical_strategy='not_available',
                                     fill_value_categorical=None,
                                     fill_value_numerical=None,
                                     numeric...
                     ('feature_select', 'passthrough'), ('fix_multi', 'passthrough'),
                     ('dfs', 'passthrough'), ('pca', 'passthrough'),
                     ['trained_model',
                      LogisticRegression(C=1.0, class_weight=None, dual=False,
                                         fit_intercept=True, intercept_scaling=1,
                                         l1_ratio=None, max_iter=1000,
                                         multi_class='auto', n_jobs=None,
                                         penalty='l2', random_state=3800,
                                         solver='lbfgs', tol=0.0001, verbose=0,
                                         warm_start=False)]],
              verbose=False),
     'champion_model_20201231.pkl')




```python
champion_model = load_model(model_name="champion_model_20201231")
```

    Transformation Pipeline and Model Successfully Loaded


## One drawback of Pycaret is the lack of clear approach of extracting and modifying output of model evaluation. 
We can go deeper with our model evaluation by using plots using PyCaret. However, it is not very clear in the documentation if one wishes to extract data used in plots. 

We can use `plot_model` to display some commonly used ones such as feature importance plot and threshold plot.


```python
plot_model(estimator=top_model[0], plot="feature")
```


![png](/assets/images/20210105/FeatureImportance.png)


The chart above shows a feature importance plot. The most important feature that predicts the target is `LoyalCH`, followed by `PriceDiff`.

```python
plot_model(estimator=top_model[0], plot="threshold")
```


![png](/assets/images/20210105/Threshold.png)


The chart above shows a threshold plot. It shows different metrics including precision, recall, and f1 score. The chart tells us that the desirable threshold of prediction probability is between 0.2 and 0.4. We can use this threshold as a cutoff in assigning a predicted label.


```python
evaluate_model(estimator=top_model[0])
```


![png](/assets/images/20210105/line85.png)


The `evaluate_model` provides all possible plots from Scikit-learn. Plot modification is cumbersome as this would mean that a user needs to go through the source Python packages to do this. In addition, testing new data (aside from holdout) requires editing of environment variables. PyCaret developers could simply include fresh data as an argument in their one of the functions.

## Conclusion
Although it is not clear how to extract data used in evaluation plots, Pycaret is promising because it reduces complexity of coding thus speeding up the data science work from data understanding to deployment. Its usage and popularity will soon rise once Pycaret garners more support from developers and contributors.

## References

- [https://pypi.org/project/pycaret/](https://pypi.org/project/pycaret/)
- [https://pycaret.org/classification/](https://pycaret.org/classification/)
